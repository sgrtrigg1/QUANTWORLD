\chapter{Deep Learning Models}
\label{ch:deep-learning}

Deep learning methods offer flexible architectures capable of modelling the complex, nonlinear and temporal dependencies frequently observed in financial markets. Unlike classical machine-learning models, deep neural networks learn hierarchical representations directly from data, reducing the need for extensive manual feature engineering. This chapter introduces the deep-learning models implemented in this dissertation—Artificial Neural Networks (ANNs) and Long Short-Term Memory (LSTM) networks—and evaluates their strengths, limitations and suitability for financial forecasting.

\section{Motivation for Deep Learning in Quantitative Finance}
Financial markets are characterised by non-linearity, regime shifts, volatility clustering and long-range temporal dependencies. Classical models such as OLS are limited in their ability to capture these behaviours because they impose fixed linear relationships. Even more flexible methods—such as logistic regression or tree-based models—rely heavily on engineered features and often struggle with sequential structure.

Deep learning addresses these challenges through several key capabilities:

\subsection{Automatic Feature Learning}
Deep networks can learn meaningful representations such as trend structure, volatility interactions or cyclical behaviour directly from raw or minimally processed data. This reduces dependence on technical indicators or bespoke feature transformations.

\subsection{Modelling Nonlinear Dynamics}
Neural networks are universal function approximators and can model arbitrary nonlinear relationships between returns, volatility, macroeconomic indicators and latent market factors.

\subsection{Handling Sequential Dependencies}
Time-series architectures such as LSTMs are specifically designed to capture long-range temporal dependencies and memory effects, which are common in markets (e.g.\ momentum, mean reversion, volatility spillover).

\subsection{Integration of Heterogeneous Data}
Deep models naturally integrate heterogeneous inputs—prices, volumes, macroeconomic signals, textual sentiment—making them suitable for multimodal forecasting tasks in modern quantitative finance.

Despite these advantages, deep learning is not guaranteed to outperform simpler models in daily financial forecasting. Noise dominance, structural breaks and limited data availability often restrict the practical performance of high-capacity architectures. These limitations are addressed later in Section~\ref{sec:dl-limitations}.

\section{Artificial Neural Networks (ANNs)}
Artificial Neural Networks represent the simplest form of deep-learning models and provide an accessible baseline for nonlinear forecasting.

\subsection{Architecture}
The ANN architecture used in this dissertation consists of:
\begin{itemize}
    \item an input layer representing engineered features,
    \item one or two hidden layers with non-linear activation functions (ReLU or \texttt{tanh}),
    \item a single output neuron producing a one-day-ahead price or return prediction.
\end{itemize}

The model is trained using backpropagation with a mean squared error (MSE) loss function.

\subsection{Training Procedure}
The ANN was trained on:
\begin{itemize}
    \item scaled returns, volatility measures and technical indicators,
    \item a consistent train/test split across all models,
    \item batch gradient descent optimisation,
    \item early stopping to prevent overfitting.
\end{itemize}

\subsection{Performance and Interpretation}
Empirically, the ANN demonstrates greater flexibility than OLS and can approximate nonlinear behaviours. However:
\begin{itemize}
    \item it remains sensitive to noise,
    \item performance improvements over OLS are modest,
    \item prediction variance increases during volatile periods.
\end{itemize}

Overall, the ANN serves as a stepping stone toward more specialised sequential models such as LSTMs.

\section{Long Short-Term Memory (LSTM) Networks for Sequential Forecasting}
Long Short-Term Memory (LSTM) networks are designed to capture sequential dependencies in time-series data. They mitigate the vanishing-gradient problem of standard RNNs through gated mechanisms that control information flow.

\subsection{Architecture}
\label{sec:lstm-architecture}

The LSTM model implemented in this dissertation consists of:
\begin{itemize}
    \item one LSTM layer with 16--32 units,
    \item dropout regularisation,
    \item a dense output layer predicting next-day price or return.
\end{itemize}

LSTMs operate on fixed-length sliding windows of sequential data (e.g.\ 30-day windows), enabling them to learn temporal structure rather than treating inputs as independent observations.

\subsection{Training Setup}
The LSTM was trained using:
\begin{itemize}
    \item normalised returns and engineered features,
    \item fixed-length look-back windows,
    \item the Adam optimiser,
    \item 50--100 epochs with early stopping.
\end{itemize}

\subsection{Empirical Observations}
In line with the literature, the LSTM model:
\begin{itemize}
    \item captures short-term temporal structure effectively,
    \item adapts well to trending regimes,
    \item struggles in sideways or noisy markets,
    \item demonstrates no dramatic outperformance over simpler models on daily stock data.
\end{itemize}

This outcome is academically meaningful: daily stock prices are highly noisy, and their predictability is limited. LSTMs tend to outperform classical models only when:
\begin{itemize}
    \item strong temporal patterns exist,
    \item large datasets are available (e.g.\ intraday or millisecond-level data),
    \item repetitive microstructural behaviour is present.
\end{itemize}

Thus, the empirical results reinforce that model complexity must be matched to signal strength.

\section{Comparison of Deep Learning with Classical Models}
This dissertation compares the performance of OLS, ANN and LSTM models under identical experimental conditions.

\subsection{Interpretability vs.\ Complexity}
\begin{itemize}
    \item OLS is fully interpretable but exhibits high bias.
    \item ANNs capture nonlinearities with limited interpretability.
    \item LSTMs learn temporal structure but are the least interpretable.
\end{itemize}

In regulated domains, interpretability remains a competitive advantage for simpler models.

\subsection{Forecasting Accuracy}
Across experiments:
\begin{itemize}
    \item deep models provide small improvements in specific regimes,
    \item LSTMs outperform ANNs when sequential patterns are strong,
    \item classical models remain competitive during noisy or sideways periods.
\end{itemize}

This supports a key conclusion: financial prediction is often limited by the data, not the model.

\subsection{Robustness and Stability}
\begin{itemize}
    \item OLS is stable but rigid,
    \item ANN is flexible but prone to overfitting,
    \item LSTM is adaptive but sensitive to window length and hyperparameters.
\end{itemize}

Rolling-window OLS frequently matches or exceeds LSTM performance during regime changes due to its rapid local adaptability.

\section{Limitations of Deep Learning in Finance}
\label{sec:dl-limitations}

Deep learning faces several structural challenges in financial applications:

\subsection{Limited Data Volume}
Daily time series contain relatively few observations, limiting the ability of large networks to generalise.

\subsection{Non-Stationarity}
Market structure evolves over time, violating the stability assumptions that many neural networks depend on.

\subsection{Noise Dominance}
A substantial portion of asset-price movements is random, making it difficult for networks to learn persistent patterns.

\subsection{Overfitting Risk}
High-parameter models may capture noise rather than signal, leading to misleading in-sample performance.

\subsection{Computational Cost}
Deep models—especially those trained with rolling or walk-forward validation—are computationally intensive compared with classical approaches.

These limitations highlight the importance of combining deep learning with domain knowledge, robust validation and strong benchmark comparisons.

\section{Applications of Deep Learning in Quantitative Finance}
Although deep learning has limitations, it remains highly effective in areas with richer structure, including:
\begin{itemize}
    \item high-frequency trading and microstructure modelling (LSTMs, CNNs),
    \item sentiment analysis (FinBERT, GPT-based models),
    \item volatility forecasting (hybrid LSTM--GARCH architectures),
    \item multimodal modelling combining text, price and macroeconomic data,
    \item order-book prediction and trade execution,
    \item portfolio allocation using transformer-based models,
    \item reinforcement-learning-based decision policies.
\end{itemize}

These applications demonstrate that deep learning is not a replacement for classical methods, but rather a complementary tool that excels under the right conditions.
